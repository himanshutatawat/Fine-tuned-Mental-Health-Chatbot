# Fine-Tuned Mental Health Chatbot (Phi-2)

## ğŸš€ Project Overview
This project focuses on fine-tuning the different models to build an AI-powered **mental health chatbot**. The chatbot is designed to provide supportive and empathetic responses to users seeking guidance. 

By leveraging **LoRA (Low-Rank Adaptation)** and **4-bit quantization**, the model is optimized for efficient training and deployment while maintaining high-quality responses.

---

## ğŸ”¥ Features
âœ… Fine-tuned different models for mental health conversations  
âœ… **LoRA fine-tuning** for efficient adaptation  
âœ… **4-bit quantization** for faster inference and low memory usage  
âœ… Uses **Gradient Checkpointing** for improved training efficiency  
âœ… Built using **Hugging Face Transformers, PEFT, and TRL**  

---

## ğŸ“‚ Tech Stack
- **Deep Learning Frameworks**: PyTorch, Transformers  
- **Fine-Tuning Methods**: LoRA, BitsAndBytes  
- **Optimization Techniques**: Quantization, Gradient Checkpointing  
- **Dataset**: Mental Health Counseling Conversations  
- **Deployment Possibilities**: API, Chatbot UI, Edge Devices  

---

## ğŸ”§ Setup & Installation
1ï¸âƒ£ Clone the repository:
```sh
git clone https://github.com/yourusername/fine-tuned-mental-health-chatbot.git
cd fine-tuned-mental-health-chatbot
```
2ï¸âƒ£ Install dependencies:

```sh
pip install torch peft bitsandbytes transformers trl accelerate einops tqdm scipy pandas datasets
```
3ï¸âƒ£ Login to Hugging Face:

```sh
from huggingface_hub import interpreter_login
interpreter_login()
```
4ï¸âƒ£ Train or load the model for inference.
