# -*- coding: utf-8 -*-
"""finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-FGEUNS-IauwGPM_RDFtTUpIckgA2_CG
"""

pip install -q torch peft bitsandbytes transformers trl accelerate einops tqdm scipy lamini datasets

import lamini
from lamini import Lamini

lamini.api_key = "your_lamini_api_key"  # ðŸ”¥ Replace with your actual Lamini API key

import json
import pandas as pd
import lamini
from lamini import Lamini
from datasets import load_dataset


# âœ… Load Dataset
from datasets import load_dataset

dataset = load_dataset("xDAN-datasets/ChatDoctor_chatdoctor_7k",split="train")
#dataset = load_dataset("Amod/mental_health_counseling_conversations", split="train")
df = pd.DataFrame(dataset)

# âœ… Format Data for Fine-Tuning
formatted_data = [
    {
        "input": row["input"],
        "output": row["output"]
    }
    for _, row in df.iterrows()
]

# âœ… Save to JSON (Optional)
with open("formatted_dataset.json", "w", encoding="utf-8") as f:
    json.dump(formatted_data, f, indent=4)

# âœ… Initialize Lamini Model
llm = Lamini(model_name="meta-llama/Llama-3.2-1B-Instruct")

# âœ… Fine-Tune the Model with Data
llm.tune(
    data_or_dataset_id=formatted_data,
    finetune_args = {
    "learning_rate": 2e-4,  # Lower LR for stability
    "num_train_epochs": 4,  # Can increase if underfitting
    "batch_size": 8,  # Adjust based on GPU memory (reduce if OOM)
    "gradient_accumulation_steps": 4,  # Effective batch size = 8*4 = 32
    "optim": "adamw_torch_fused",  # Best for performance
    "lr_scheduler_type": "cosine",  # Cosine decay for better convergence
    "warmup_ratio": 0.03,  # 3% of total steps as warmup
    "weight_decay": 0.1,  # Regularization to prevent overfitting
    "fp16": True,  # Enable mixed precision (if using FP16 training)
    "save_steps": 500,  # Save checkpoints every 500 steps
    "logging_steps": 100,  # Log progress every 100 steps
    "evaluation_strategy": "steps",
    "eval_steps": 500,  # Evaluate every 500 steps
    "save_total_limit": 2,  # Keep last 2 checkpoints
    "load_best_model_at_end": True,  # Auto-load best checkpoint
}

)

print("ðŸ”¥ Model fine-tuning started on Lamini!")

